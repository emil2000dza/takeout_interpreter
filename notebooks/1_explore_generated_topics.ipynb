{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c500516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# add project root to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.db.snowflake_client import SnowflakeORM\n",
    "from src.db.tables import DiscoveredTopics\n",
    "\n",
    "client_snowflake = SnowflakeORM()\n",
    "\n",
    "with client_snowflake.session_scope() as session:\n",
    "        query = session.query(\n",
    "            DiscoveredTopics.topic_name,\n",
    "            DiscoveredTopics.description,\n",
    "        )\n",
    "        all_topics = query.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdb266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# add project root to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.db.snowflake_client import SnowflakeORM\n",
    "from src.db.tables import ChromeHistory\n",
    "\n",
    "client_snowflake = SnowflakeORM()\n",
    "\n",
    "with client_snowflake.session_scope() as session:\n",
    "        query = session.query(\n",
    "            ChromeHistory.title,\n",
    "            ChromeHistory.url,\n",
    "        )\n",
    "        all_topics = query.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e0c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_topics).groupby('url').count()\n",
    "df.sort_values('title', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.topic_modeling.gemini import call_llm \n",
    "from src.topic_modeling.prompts import TOPIC_REFINMENT_PROMPT\n",
    "from src.topic_modeling.topic_discovery import extract_json\n",
    "import json\n",
    "\n",
    "def format_topics(topics):\n",
    "    \"\"\"Format a list of (title, description) into a single structured text block.\"\"\"\n",
    "    lines = []\n",
    "    for i, (title, desc) in enumerate(topics, start=1):\n",
    "        lines.append(f\"Topic {i}:\\n- Name: {title}\\n- Description: {desc}\\n\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "prompt = TOPIC_REFINMENT_PROMPT.format(\n",
    "    all_topics=format_topics(all_topics)\n",
    "    )\n",
    "response = call_llm(prompt)\n",
    "\n",
    "try:\n",
    "    topics_dict = extract_json(response)\n",
    "    print(topics_dict)\n",
    "except json.JSONDecodeError:\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c997e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download NLTK resources if needed\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Clean, remove stopwords, and lemmatize text.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # keep only letters\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def split_topics(data):\n",
    "    \"\"\"Split topic names on '&' and duplicate rows with same description.\"\"\"\n",
    "    new_data = []\n",
    "    for topic, desc in data:\n",
    "        parts = [part.strip() for part in topic.split(\"&\")]\n",
    "        for part in parts:\n",
    "            new_data.append((part, desc))\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def analyze_ngrams(all_topics, ngram_range=(1, 3), top_k=20):\n",
    "    \"\"\"\n",
    "    Compute TF-IDF scores for n-grams in topic names.\n",
    "    Returns ranked list of top n-grams with their TF-IDF score.\n",
    "    \"\"\"\n",
    "    processed = split_topics(all_topics)\n",
    "    cleaned_topics = [preprocess_text(topic) for topic, _ in processed]\n",
    "\n",
    "    # TF-IDF on unigrams, bigrams, trigrams\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        stop_words=\"english\",\n",
    "        min_df=1,\n",
    "        max_df=0.9\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.fit_transform(cleaned_topics)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Compute average TF-IDF per term across topics\n",
    "    scores = tfidf_matrix.mean(axis=0).A1\n",
    "    tfidf_scores = dict(zip(feature_names, scores))\n",
    "\n",
    "    # Sort terms by TF-IDF score\n",
    "    top_terms = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    return top_terms, tfidf_scores\n",
    "\n",
    "\n",
    "def plot_wordcloud(tfidf_scores):\n",
    "    \"\"\"Plot a wordcloud from TF-IDF scores.\"\"\"\n",
    "    wordcloud = WordCloud(\n",
    "        width=900,\n",
    "        height=450,\n",
    "        background_color=\"white\"\n",
    "    ).generate_from_frequencies(tfidf_scores)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "top_terms, tfidf_scores = analyze_ngrams(all_topics, ngram_range=(1, 3), top_k=15)\n",
    "\n",
    "print(\"üîù Top n-grams ranked by TF-IDF:\")\n",
    "for term, score in top_terms:\n",
    "    print(f\"{term}: {score:.4f}\")\n",
    "\n",
    "# Plot wordcloud from all terms\n",
    "plot_wordcloud(tfidf_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c159ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppose your preprocessed titles and descriptions are in a list\n",
    "titles = [title for title, description in all_topics]\n",
    "descriptions = [description for title, description in all_topics]\n",
    "\n",
    "# 1Ô∏è‚É£ Create embeddings using a lightweight model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(descriptions, show_progress_bar=True)\n",
    "\n",
    "# 2Ô∏è‚É£ Apply UMAP for 2D visualization\n",
    "umap_reducer = umap.UMAP(n_neighbors=5, min_dist=0.3, random_state=42)\n",
    "embeddings_2d = umap_reducer.fit_transform(embeddings)\n",
    "# 3Ô∏è‚É£ Plot\n",
    "plt.figure(figsize=(14, 10), dpi=120)\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=50, c='skyblue', alpha=0.7)\n",
    "\n",
    "# Randomly select 50% of indices to label\n",
    "indices_to_label = random.sample(range(len(titles)), k=len(titles)//2)\n",
    "\n",
    "for i in indices_to_label:\n",
    "    plt.text(\n",
    "        embeddings_2d[i, 0]+0.01, \n",
    "        embeddings_2d[i, 1]+0.01, \n",
    "        titles[i][:25],  # first 25 chars of title\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "plt.title(\"UMAP projection of generated topic descriptions (titles shown)\", fontsize=16)\n",
    "plt.xlabel(\"UMAP 1\")\n",
    "plt.ylabel(\"UMAP 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
